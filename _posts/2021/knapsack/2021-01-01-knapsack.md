---
layout: post
title: Large Scale Knapsack Problem and an Online Solver
date: 2021-01-01
Author: Yanfei Tang
tags: [math]
comments: false
toc: false
---

&emsp;&emsp;Â Knapsack problem is widely encoutered in the real-worlds. When I was a freshman employee in the e-commerce company, this problem was the first technical problem that our team solved.

<!-- more -->

In the online advertisement or during the sales promotion, the business operator decides to give the app user coupons of different values to maximize different sales goal, such as profit, GMVs, new users and so on. In the meantime, the business operator is constrained by several factors, such as budgets, promotion duration, coupon's value limits and so on. According to the business description, there are several ways to build a model to help operator to accomplish the sales goal. Some typical models can be build on a 0/1 knapsack problem, a Markov decision process problem or even a reinforcement learning problem. These models are not castel in the air, but can be applicale in the real situations. In the following description, I will introduce a Langrangian relaxation method to solve a multidimensional knapsack problem. 

The mathematical description of the multidimesional knapsack problem is as follows,

$$
\max \sum_{i=1}^N \sum_{j=1}^{M} p_{ij}x_{ij} \\
\sum_{i=1}^N \sum_{j=1}^M b_{ijk}x_{ij} \le B_k, \quad \forall k \in [K] \\ 
\sum_{j=1}^M x_{ij} = 1, \quad \forall i \in [N] \\
x_{ij} \in \{0, 1\}, \quad \forall i \in [N], \forall j \in [M]. \tag{1}
$$

It is a typical decription for a operational research problem. We also call it the *primal* problem. The format looks a little bit terrifying at the first time. Let me explain the mathematical symbols by starting with the subscripts. The subscript $i$ means the index of participants in the promotion. $N$ is the total participant. The subscript $j$ is the index of the coupon's value. We have $M$ kinds of coupons. $p_{ij}$ describes the sensitivy or probability that the $i$th user's willing to participant the promotion given the $j$th coupon's value. This information can obtained from machine learning of user's profiles, shopping behavior and price sensitivity. $x_{ij}$ is the decision variables that needs to figure out in the solving routine. In our scenario, $x_{ij}$ can only be chosen as 0 or 1. The goal to maximize the estimates of total participancy in the promotion campaign given the constraints.

A typical contraint is the budget. In some cases, the operator also cares about the promotion campaign's efficiency, for example, return of investment (ROI). In generalization, the constraint can be summarized as follows, 

$$
\sum_{i=1}^N \sum_{j=1}^M b_{ijk}x_{ij} \le B_k, \quad \forall k \in [K], \tag{2}
$$

where $b_{ijk}$ is the expense of decision $x_{ij}$ for the $k$th constraint. For budget type constraint, if the coupon has a face value $b_j$ and total budget for the campaign is $\tilde{B}N$, the budget constraint can be written as 

$$
\sum_{i=1}^N \sum_{j=1}^M b_j x_{ij} \le \tilde{B} N . \tag{3}
$$

For the ROI constraint,

$$
\frac{\sum_{i=1}^N \sum_{j=1}^M a_{ij}p_{ij}x_{ij} }{ \sum_{i=1}^N \sum_{j=1}^M b_j p_{ij} x_{ij} }  \ge \text{ROI}  \quad \forall k \in [K], \tag{4}
$$

the numerator is the estimated revenue from the campaign and the denominator is the estimated expense of the promotion. $a_{ij}$ is the predicted price of the item that the customer might purchase given the coupon. 

Without loss of generality, we provide a Lagrangian relaxation scheme to solve the Equation (1). Every primal problem has a *dual* problem. To get the dual, We can define the Lagrangian as follows, 

$$
{\cal L}(x_{ij}, \lambda_k) = \sum_{i=1}^N \sum_{j=1}^M \Big[ p_{ij} - \sum_{k=1}^K \lambda_k b_{ijk}  \Big] x_{ij} + \sum_{k=1}^K \lambda_k B_k, \tag{5}
$$

where $\lambda_k$ is the Lagrangian multipiliers. The dual problem is

$$
\min_{\lambda_k} \max_{x_{ij}} {\cal L} (x_{ij}, \lambda_k), \\
\sum_{j=1}^M x_{ij} = 1, \quad \forall k \in [K] \\
\lambda_k \le 0 \quad k \in [M]. \\
x_{ij} \in \{0, 1\}, \quad \forall i \in [N], \forall j \in [M]. \tag{6}
$$

In typical primal-dual formulation, the equality constraint will be incorporated in the Lagrangian. In this case, we will use the equaility constraint to establish the explicit relation between $x_{ij}$ and $\lambda_k$. This is very unique in this problem. Otherwise, one has to use $\frac{\partial {\cal L}}{\partial x_{ij}} = 0$ to get the relation between decision variables and Lagrangian, for example in the algrithm of support vector machine. To elaborate this, Let us take $\lambda_k$ fixed. 

For a fixed set of Lagrangian multipliers $\lambda_k^*$, we will have the following problem, 

$$
\max_{x_{ij}} \sum_{i=1}^N \sum_{j=1}^M \Big[ p_{ij} - \sum_{k=1}^K \lambda_k^* b_{ijk}  \Big] x_{ij} + \sum_{k=1}^K \lambda_k^* B_k, \\
\sum_{j=1}^M x_{ij} = 1, \quad \forall k \in [K].  \\
x_{ij} \in \{0, 1\} \quad \forall i \in [N] \quad \forall j \in [M]. \tag{7}
$$

Now, the problem can be splited into sub-problems, such that

$$
\max_{x_{ij}} \sum_{i=1}^N \sum_{j=1}^M \Big[ p_{ij} - \sum_{k=1}^K \lambda_k^* b_{ijk}  \Big] x_{ij} \Longleftrightarrow  \sum_{i=1}^N \max_{x_{ij}} \sum_{j=1}^M \Big[ p_{ij} - \sum_{k=1}^K \lambda_k^* b_{ijk}  \Big] x_{ij} \tag{8}
$$

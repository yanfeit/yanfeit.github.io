---
layout: post
title: Large Scale Knapsack Problem and an Online Solver
date: 2021-01-01
Author: Yanfei Tang
tags: [math]
comments: false
toc: false
---

&emsp;&emsp; Knapsack problem is widely encoutered in the real-worlds. When I was a freshman employee in the e-commerce company, this problem was the first technical problem that our team solved.

<!-- more -->

In the online advertisement or during the sales promotion, the business operator decides to give the app user coupons of different values to maximize different sales goal, such as profits, GMVs, new users and so on. In the meantime, the business operator is constrained by several factors, such as budgets, promotion duration, coupon's value limits and so on. According to the business description, there are several ways to build a model to help operators to accomplish the sales goal. Some typical models can be established on basis of a 0/1 knapsack problem, a Markov decision process problem or even a reinforcement learning problem. These models are not castel in the air, but can be applicale in the real situations. In the following description, I will introduce a Lagrangian relaxation method to solve a multidimensional knapsack problem[^1]. 

# 1. Primal-Dual Formulation

## 1.1 Formalism

The mathematical description of the multidimesional knapsack problem is as follows,

$$
\max \sum_{i=1}^N \sum_{j=1}^{M} p_{ij}x_{ij} \\
\sum_{i=1}^N \sum_{j=1}^M b_{ijk}x_{ij} \le B_k, \quad \forall k \in [K] \\ 
\sum_{j=1}^M x_{ij} = 1, \quad \forall i \in [N] \\
x_{ij} \in \{0, 1\}, \quad \forall i \in [N], \forall j \in [M]. \tag{1}
$$

It is a typical decription for a operational research problem. We also call it the *primal* problem. The format looks a little bit terrifying at the first time. Let me explain the mathematical symbols by starting with the subscripts. The subscript $i$ means the index of participants in the promotion. $N$ is the total participants. The subscript $j$ is the index of the coupon's value. We have $M$ kinds of coupons. $p_{ij}$ describes the sensitivy or probability that the $i$th user's willing to participant the promotion given the $j$th coupon's value. This information can obtained from machine learning of user's profiles, shopping behavior and price sensitivity. $x_{ij}$ is the decision variables that needs to be figured out in the solving routines. In our scenario, $x_{ij}$ can only be chosen as 0 or 1. The goal is to maximize the estimates of total participancy in the promotion campaign given the constraints.

A typical contraint is the budget. In some cases, the operator also cares about the promotion campaign's efficiency, for example, return of investment (ROI). In generalization, the constraint can be summarized as follows, 

$$
\sum_{i=1}^N \sum_{j=1}^M b_{ijk}x_{ij} \le B_k, \quad \forall k \in [K], \tag{2}
$$

where $b_{ijk}$ is the expense of decision $x_{ij}$ for the $k$th constraint. For budget type constraint, if the coupon has a face value $b_j$ and total budget for the campaign is $\tilde{B}N$, the budget constraint can be written as 

$$
\sum_{i=1}^N \sum_{j=1}^M b_j p_{ij} x_{ij} \le \tilde{B} N . \tag{3}
$$

For the ROI constraint,

$$
\frac{\sum_{i=1}^N \sum_{j=1}^M a_{ij}p_{ij}x_{ij} }{ \sum_{i=1}^N \sum_{j=1}^M b_j p_{ij} x_{ij} }  \ge \text{ROI}  \quad \forall k \in [K], \tag{4}
$$

the numerator is the estimated revenue from the campaign and the denominator is the estimated expense of the promotion. $a_{ij}$ is the predicted price of the item that the customer might purchase given the coupon. 

Without loss of generality, we provide a Lagrangian relaxation scheme to solve the Equation (1). Every primal problem has a *dual* problem. To get the dual, We can define the Lagrangian as follows, 

$$
{\cal L}(x_{ij}, \lambda_k) = \sum_{i=1}^N \sum_{j=1}^M \Big[ p_{ij} - \sum_{k=1}^K \lambda_k b_{ijk}  \Big] x_{ij} + \sum_{k=1}^K \lambda_k B_k, \tag{5}
$$

where $\lambda_k$ is the Lagrangian multipiliers. The dual problem is

$$
\min_{\lambda_k} \max_{x_{ij}} {\cal L} (x_{ij}, \lambda_k), \\
\sum_{j=1}^M x_{ij} = 1, \quad \forall k \in [K] \\
\lambda_k \ge 0 \quad k \in [M]. \\
x_{ij} \in \{0, 1\}, \quad \forall i \in [N], \forall j \in [M]. \tag{6}
$$

In typical primal-dual formulation, the equality constraint will be incorporated in the Lagrangian. In this case, we will use the equaility constraint to establish the explicit relation between $x_{ij}$ and $\lambda_k$. This is very unique in this problem. Otherwise, one has to use $\frac{\partial {\cal L}}{\partial x_{ij}} = 0$ to get the relation between decision variables and Lagrangian multipilers, for example in the algrithm of support vector machine. To elaborate this, Let us take $\lambda_k$ fixed. 

For a fixed set of Lagrangian multipliers $\lambda_k^*$, we will have the following problem, 

$$
\max_{x_{ij}} \sum_{i=1}^N \sum_{j=1}^M \Big[ p_{ij} - \sum_{k=1}^K \lambda_k^* b_{ijk}  \Big] x_{ij} + \sum_{k=1}^K \lambda_k^* B_k, \\
\sum_{j=1}^M x_{ij} = 1, \quad \forall k \in [K].  \\
x_{ij} \in \{0, 1\} \quad \forall i \in [N] \quad \forall j \in [M]. \tag{7}
$$

Now, the problem can be splited into sub-problems, such that

$$
\max_{x_{ij}} \sum_{i=1}^N \sum_{j=1}^M \Big[ p_{ij} - \sum_{k=1}^K \lambda_k^* b_{ijk}  \Big] x_{ij} \Longleftrightarrow  \sum_{i=1}^N \max_{x_{ij}} \sum_{j=1}^M \Big[ p_{ij} - \sum_{k=1}^K \lambda_k^* b_{ijk}.  \Big] x_{ij} \tag{8}
$$

Here we neglect the constant part $\sum_{k=1}^K \lambda_k^* B_k$. The maximization of the overall revenue constrained by resource limits becomes the maximization of every customer's expense subjected by resource limits. The optimal solution of the decision variables for every customer is,

$$
x_{ij} = 1, \quad j = \arg \max_{j} p_{ij} - \sum_{k=1}^K \lambda_k^* b_{ijk}, \\
\text{otherwise} \quad x_{ij} = 0. \tag{9}
$$

Equation.(9) is the so-called relation betweeen decision variables and Lagrangian multipliers, $x_{ij}(\lambda)$. With that, we can use it in the following minimization procedures, 

$$
\min_{\lambda_k} {\cal L} (\lambda_k)  \\
\text{s.t.} \quad \lambda_k \ge 0, \forall k \in [K] \tag{10}
$$

We can solve it by a gradient descent method or a stochastic descent method. It is noticed that $x_{ij}(\lambda)$ has a singularity. This singularity is ignored by providing the gradient of ${\cal L}$ with respect $\lambda_r$,

$$
\frac{\partial {\cal L}}{\partial \lambda_r} = -\lambda_r \sum_i^N \sum_j^M b_{ijr} x_{ij} + \lambda_r B_r. \tag{11}
$$

In the following case, we used Adam optimizer to find out the approximate optimal solution[^2]. In order to evaluate the solution, python-MIP package which applies Coin-or branch and cut algorithm is used to get the true optimal value to evalute the gap. The optimality is defined as, 

$$
\text{Optimality} = 1 - \frac{Q - Q^*}{Q^*}. \tag{12}
$$

where $Q^*$ is the objective value of optimal solution from an Exact algorithm. $Q$ is the objective value of the nearly optimal solution from the approximate method. Besides, the dual descent method does not necessarily guarantee the constraints. To evaluate the constraints satisfaction, the following metric is used

$$
\text{SAT} = \sum_{k=1}^K \frac{\max(\sum_i^N \sum_j^N b_{ijk}x_{ij} - B_k,0)}{\| B_k \| } \tag{13}
$$

## 2.2 Online Inference

For an online application, customers will be served one by one. The responding time is very limited to several microseconds. The sensitivity $p_{ij}$ is predicted by inference of the machine learning model. Equation.(9) can be utilized in the inference engine to decide which coupon can be used for the customer.


# 2. Simulated Case Study
## 2.1 Online Flow Control

In online advertisement, the web owner have $M$ advertisements to distribute. In order to optimize the total click rate, the problem can be formalize as follows,

$$
\max \sum_{i=1}^N \sum_{j=1}^{M} p_{ij}x_{ij} \\
\sum_{i=1}^N x_{ij} \le s_j N, \quad \forall j \in [M] \\ 
\sum_{j=1}^M x_{ij} = 1, \quad \forall i \in [N] \\
x_{ij} \in \{0, 1\}, \quad \forall i \in [N], \forall j \in [M]. \tag{11}
$$

where $s_j$ represents the ratio of the $j$th advertisements and $p_{ij}$ is the estimated click rate of the $j$th advertisements for the $i$th user. The Lagrangian of the Equation.(1) and its derivative are, 

$$
{\cal L}(\lambda_j, x_{ij}) =  \sum_{i=1}^N \sum_{j=1}^M \Big[ p_{ij} - \lambda_j \Big] x_{ij} + \sum_{j=1}^M \lambda_j s_j N, \tag{12}
$$

and

$$
\frac{\partial {\cal L}}{\partial \lambda_r} = -\sum_i x_{ir} + s_r N. \tag{13}
$$

## 2.2 Simulated Experiment




[^1]: Zhang, X., Qi, F., Hua, Z. & Yang, S. Solving Billion-Scale Knapsack Problems. *in Proceedings of The Web Conference 2020* 3105–3111 (ACM, 2020).

[^2]: Kingma, D. P. & Ba, J. Adam: A Method for Stochastic Optimization. *arXiv* 1412.6980 (2017).


---
layout: post
title: Tips about Linux and Programming
date: 2022-12-08
Author: Yanfei Tang
tags: [ÊùÇÊñá]
comments: false
toc: true
---

&emsp;&emsp;¬†Some useful tips about Linux commands, programming skills and else. All of the tips are purely taken from the website. I am continuing add these tips to save my bad memory. üòÇ

<!-- more -->

# Linux Commands

### grep

[Find all files containing specific text (string) on Linux](https://stackoverflow.com/questions/16956810/how-to-find-all-files-containing-specific-text-string-on-linux)

```bash
grep -rnw '/path/to/somewhere' -e 'pattern'
```

- <code>-r</code> or <code>-R</code> is recursive,
- <code>-n</code> is line number, and
- <code>-w</code> stands for match the whole word.
- <code>-l</code> (lower-case L) can be added to just give the file name of matching files.
- <code>-e</code> is the pattern used during the search

Along with these, <code>--exclude</code>, <code>--include</code>, <code>--exclude-dir</code> flags could be used for efficient searching:

- This will only searching through those files which have .c or .h extensions:

```bash
grep --include=\*.{c,h} -rnw '/path/to/somewhere/' -e "pattern"
```

- This will exclude searching all the files ending with .o extension:

```bash
grep --exclude=\*.o -rnw '/path/to/somewhere/' -e "pattern"
```

- For directories it's possible to exclude one or more directories using the <code>--exclude-dir</code> parameter. For example, this will exclude the dirs <code>dir1/</code>, <code>dir2/</code> and all of them matching <code>*.dst/</code>:

```bash
grep --exclude-dir={dir1,dir2,*.dst} -rnw '/path/to/search/' -e "pattern"
```

### sshpass

If we want to ssh into the serve with a single command line, we can use <code>sshpass</code>. To get command by

```bash
sudo apt-get install sshpass
```

Example:

```bash
sshpass -p 'mySSHPasswordHere' ssh username@server.nixcraft.net.in
```

### rsync

*Rsync*, which stands for *remote sync*, is a remote and local file synchronization tool. It uses an algorithm to minimize the amount of data copied by only moving the portions of files that have changed.

```shell
rsync -azvP source destination
# example
rsync -azvP yanfeit@ipaddress:path/to/folder ./
```

### ssh key login

Key authentication is widely used and recommended. The user generates the public key and private key on the client by using some algorithms, e.g., RSA/ED25519/DAS/ECDSA. For the first time, the user sends the public key to the server. When the user logins into the server, the server generates an encrypted random number by the public key. The server sends back the encrypted number. After the client received the encrypted data, it decrypts the data by using the private key. The client returns the decrypted information. At last, the server verifies whether the decrypted information is correct or not.

<p align="center">
   <img src="/images/2022/tips/sshlogin.png" alt="drawing" align="middle"/>
   <em>Fig. The principle of ssh key login</em>
</p>

Below is an example showing how to use ssh key login method.

ssh-keygen -t ed25519 # here I use ED25519 algorithm to generate the public key and private key

Typically, we can store the keys in the default folder ~/.ssh/. Then the public key is sent to the server.
```bash
ssh-copy-id -i ~/.ssh/id_ed25519.pub your_account_name@server1_ip
```
You will be asked to enter the password to verify the data transmission.

Do this again on the second server.
```bash
ssh-copy-id -i ~/.ssh/id_ed25519.pub your_account_name@server2_ip
```

### split and cat large files

We can use the <code>split</code> command to split files, which supports both text and binary file splitting, and the <code>cat</code> command to merge files.

When splitting a file by the file size, we need to specify the split file size with the <code>-b</code> argument specifies the size of the split file.

```bash
split -b 4096m large_file.tar.gz small_file
```

We can use the <code>cat</code> command to merge files that are split in the above ways.
```bash
cat small_file* > original_file
```


# Sphinx

```shell
make html # create html files
cp -r build/html/. /home/nginx/html # move html files to nginx file
```




# Programming

Read arguments from the input is useful. Below is a typical method to read arguments in C programming.

```c++
for(int i = 0; i < argc; i++) {
  if((strcmp(argv[i], "-i") == 0) || (strcmp(argv[i], "--input_file") == 0)) {
    input_file = argv[++i];
    continue;
    }
}
```





# Message Passing Interface

I take the tips from the website [mpitutorial](https://mpitutorial.com/tutorials/mpi-hello-world/). 



## Basic 

```c++
MPI_Init(int* argc, char** argv)
```

During <code>MPI_Init</code>, all of MPI's global and internal variables are constructed. For example, a communicator is formed around all of the processes that were spawned, and unique ranks are assigned to each process.



```c++
MPI_Comm_size(MPI_Comm communicator, int* size)
```

<code>MPI_Comm_size</code> returns the size of a communicator. For example, 

```c++
MPI_Comm_size(MPI_COMM_WORLD, &world_size);
```

<code>MPI_COMM_WORLD</code> encloses all of the processes in the job. Therefore, this call should return the amount of processes that were requested for the job.



```c++
MPI_Comm_rank(MPI_Comm communicator, int* rank)
```

 <code>MPI_Comm_rank</code> returns the rank of a process in a communicator. Each process inside of a communicator is assigned an incremental rank starting from **zero**. 



A miscellaneous and less-used function is:

```c++
MPI_Get_processor_name(char* name, int* name_length)
```

<code>MPI_Get_processor_name</code> obtains the actual name of the processor on which the process is executing. 



```c++
MPI_Finalize()
```

<code>MPI_Finalize()</code> is used to clean up the MPI environment. No more MPI calls can be made after this one.



## Send and Receive

```c++
MPI_Send(void* data, int count, MPI_Datatype datatype, int destination, int tag, MPI_Comm communicator)

MPI_Recv(void* data, int count, MPI_Datatype datatype, int source,      int tag, MPI_Comm communicator, MPI_Status* status)
```

The first argument is data buffer. The second and third arguments describes the count and type of elements that reside in the buffer. <code>MPI_Send</code> sends the exact count of elements, and <code>MPI_Recv</code> will receive **at most** the count of elements. The fourth and fifth arguments specify the rank of the sending/receiving process and the tag of the message. The sixth argument specifies the communicator and the last argument (for <code>MPI_Recv</code> only) provides information about the received message. 



## MPI_Status and MPI_Probe

The three primary pieces of information from <code>MPI_Status</code> includes:

1. **The rank of the sender**. For example, if we declare an <code>MPI_Status stat</code> variable, the rank can be accessed with <code>stat.MPI_SOURCE</code>. 
2. **The tag of the message**. The tag of the message can be accessed by the <code>MPI_TAG</code> element of the structure (similar to <code>MPI_SOURCE</code>).
3. **The length of the message**.

```c++
MPI_Get_count(MPI_Status* status, MPI_Datatype datatype, int* count)
```

In <code>MPI_Get_count</code>, the user passes the <code>MPI_Status</code> structure, the <code>datatype</code> of the message, and <code>count</code> is returned. The <code>count</code> variable is the total number of <code>datatype</code> elements that were received. 

We can use <code>MPI_Probe</code> to query the message size before actually receiving it. The function prototype looks like this.

```c++
MPI_Probe(int source, int tag, MPI_Comm comm, MPI_Status* status)
```

<code>MPI_Probe</code> looks quite similar to <code>MPI_Recv</code>. In fact, you can think of <code>MPI_Probe</code> as an <code>MPI_Recv</code> that does everything but receive the message. Similar to <code>MPI_Recv</code>, <code>MPI_Probe</code> will block for a message with a matching tag and sender. When the message is available, it will fill the status structure with information. The user can then use <code>MPI_Recv</code> to receive the actual message. 



## Broadcast

```c++
MPI_Barrier(MPI_Comm communicator)
```

<p align="center">
   <img src="/images/2022/tips/broadcast_pattern.png" alt="drawing" align="middle"/>
   <em>Fig. The communication pattern of a broadcast</em>
</p>

```c++
MPI_Bcast(void* data, int count, MPI_Datatype datatype, int root, MPI_Comm communicator)
```

## MPI Scatter, Gather, and Allgather

<p align="center">
   <img src="/images/2022/tips/broadcastvsscatter.png" alt="drawing" align="middle"/>
   <em>Fig. The communication pattern of a scatter</em>
</p>

```c++
MPI_Scatter(void* send_data, int send_count, MPI_Datatype send_datatype, void* recv_data,
            int recv_count, MPI_Datatype recv_datatype, int root, MPI_Comm communicator)
```

The first parameter, <code>send_data</code>, is an array of data that resides on the root process. The second parameter and third parameters, <code>send_count</code> and <code>send_datatype</code>, dictate how many elements of a specific MPI Datatype will be sent to each process. If <code>send_count</code> is one and <code>send_datatype</code> is <code>MPI_INT</code>, then process zero gets the first integer of the array, process one gets the second integer, and so on. If <code>send_count</code> is two, the process zero gets the first and second integers, process one gets the third and fourth, and so on. The <code>recv_data</code> parameter is a buffer of data that can hold <code>recv_count</code> elements that have a datatype of <code>recv_datatype</code>. The last parameter, <code>root</code> and <code>communicator</code>, indicate the root process that is scattering the array of data and the communicator in which the processes reside. 



<p align="center">
   <img src="/images/2022/tips/gather.png" alt="drawing" align="middle"/>
   <em>Fig. The communication pattern of a gather</em>
</p>

```c++
MPI_Gather(void* send_data, int send_count, MPI_Datatype send_datatype,void* recv_data,
    int recv_count, MPI_Datatype recv_datatype, int root, MPI_Comm communicator)
```

In <code>MPI_Gather</code>, only the root process needs to have a valid receive buffer. All other calling processes can pass <code>NULL</code> for <code>recv_data</code>. Also, don‚Äôt forget that the *recv_count* parameter is the count of elements received *per process*, not the total summation of counts from all processes. 



<p align="center">
   <img src="/images/2022/tips/allgather.png" alt="drawing" align="middle"/>
   <em>Fig. The communication pattern of a Allgather</em>
</p>

```c++
MPI_Allgather(void* send_data, int send_count, MPI_Datatype send_datatype,
    void* recv_data, int recv_count, MPI_Datatype recv_datatype, MPI_Comm communicator)
```

The function declaration for <code>MPI_Allgather</code> is almost identical to <code>MPI_Gather</code> with the difference that there is no root process in <code>MPI_Allgather</code>.



## MPI Reduce and Allreduce

<p align="center">
   <img src="/images/2022/tips/mpi_reduce_2.png" alt="drawing" align="middle"/>
   <em>Fig. The communication pattern of a reduce</em>
</p>

```c++
MPI_Reduce(void* send_data, void* recv_data, int count, MPI_Datatype datatype,
    MPI_Op op, int root, MPI_Comm communicator)
```

The <code>send_data</code> parameter is an array of elements of type <code>datatype</code> that each process wants to reduce. The <code>recv_data</code> is only relevant on the process with a rank of <code>root</code>. The <code>recv_data</code> array contains the reduced result and has a size of <code>sizeof(datatype) * count</code>. The <code>op</code> parameter is the operation that you wish to apply to your data.

<p align="center">
   <img src="/images/2022/tips/mpi_allreduce_1.png" alt="drawing" align="middle"/>
   <em>Fig. The communication pattern of a Allreduce</em>
</p>

```c++
MPI_Allreduce(void* send_data, void* recv_data, int count,
    MPI_Datatype datatype, MPI_Op op, MPI_Comm communicator)
```



# OpenMP

Here we will introduce the basic instructions for OpenMP. In C/C++, the format for the instructions is, 

```c++
#pragma omp instrct [clause [clause]...]
```

For example, 

```c++
#pragma omp parallel private(i,j)
```

<code>parallel</code> is the instruction, <code>private</code> is the clause.



## OpenMP instruction sets

| instructions      | details                                                      |
| ----------------- | ------------------------------------------------------------ |
| parallel          | indicates the following codes will be executed in parallel   |
| for               | used before the for loop. The for loop is assigned in multiple threads to be executed in parallel. The irrelevances between the loops is guaranteed. |
| parallel for      | a combination of  parallel and for instructions, used before the for loops which indicates the for loops will be executed in multiple threads in parallel. |
| sections          | used before the codes that might be executed in parallel.    |
| parallel sections | a combination of parallel and sections.                      |
| critical          | used before a critical region of the code                    |
| single            | indicates the following codes shall be executed in a single thread. |
| flush             | to ensure that the temporary memory is consistent with the actual memory. That is, the shared memory is consistent in all the threads. |
| barrier           | all the threads stop when they meet barrier instruction.     |
| atomic            | assign one memory to update.                                 |
| master            | assign codes to be executed in the main pragram.             |
| ordered           |                                                              |
| threadprivate     |                                                              |
|                   |                                                              |
|                   |                                                              |
|                   |                                                              |
|                   |                                                              |
|                   |                                                              |
|                   |                                                              |

